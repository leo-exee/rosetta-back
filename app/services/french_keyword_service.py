import json
import logging
import os
import re
from collections import Counter, defaultdict

import spacy
from textstat import flesch_reading_ease

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FrenchKeywordExtractor:
    def __init__(self, model_name="fr_core_news_sm"):
        """
        Initialise l'extracteur avec spaCy fran√ßais
        Args:
            model_name: Mod√®le spaCy fran√ßais (fr_core_news_sm, fr_core_news_md, fr_core_news_lg)
        """
        try:
            self.nlp = spacy.load(model_name)
            logger.info(f"‚úÖ Mod√®le spaCy fran√ßais '{model_name}' charg√© avec succ√®s")
        except OSError:
            logger.error(f"‚ùå Mod√®le spaCy fran√ßais '{model_name}' non trouv√©")
            logger.info("üí° Installez-le avec: python -m spacy download fr_core_news_sm")
            raise

        # Configuration pour l'extraction fran√ßaise
        self.min_word_length = 3
        self.max_word_length = 25  # Fran√ßais a des mots plus longs
        self.min_frequency = 2

        # POS tags fran√ßais √† conserver pour les mots-cl√©s
        self.valid_pos = {"NOUN", "PROPN", "ADJ", "VERB"}

        # Mots-outils fran√ßais √† ignorer (stop words personnalis√©s)
        self.french_blacklist = {
            # Verbes tr√®s courants
            "√™tre", "avoir", "faire", "dire", "aller", "voir", "savoir", "prendre",
            "venir", "vouloir", "pouvoir", "falloir", "devoir", "croire", "trouver",
            "donner", "parler", "aimer", "porter", "laisser", "entendre", "demander",
            "rester", "passer", "arriver", "entrer", "monter", "sortir", "partir",
            "tenir", "finir", "jouer", "tourner", "servir", "ouvrir", "mettre",

            # Noms tr√®s g√©n√©riques
            "ann√©e", "temps", "jour", "moment", "fa√ßon", "chose", "cas", "part",
            "lieu", "place", "fois", "point", "nombre", "partie", "c√¥t√©", "main",
            "gens", "homme", "femme", "enfant", "personne", "monde", "pays",
            "ville", "maison", "√©cole", "travail", "probl√®me", "question", "vie",

            # Adjectifs tr√®s courants
            "nouveau", "premier", "dernier", "long", "grand", "petit", "autre",
            "vieux", "beau", "gros", "jeune", "bon", "mauvais", "fran√ßais",
            "national", "international", "public", "priv√©", "social", "politique",
            "√©conomique", "important", "diff√©rent", "possible", "certain",

            # D√©terminants et pronoms non capt√©s par spaCy
            "tout", "tous", "toute", "toutes", "chaque", "plusieurs", "quelque",
            "aucun", "m√™me", "tel", "cette", "cela", "celui", "celle",
        }

    def clean_french_text(self, text: str) -> str:
        """Nettoie le texte fran√ßais avant traitement"""
        # Supprimer les URLs
        text = re.sub(r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+])+", "", text)
        # Supprimer les emails
        text = re.sub(r"\S+@\S+", "", text)
        # Supprimer les caract√®res sp√©ciaux mais garder les accents fran√ßais
        text = re.sub(r"[^\w\s\.\!\?\,\;\:\√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ø√ß]", " ", text)
        # Normaliser les espaces
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    def extract_french_keywords(self, text: str, top_k: int = 30) -> list[dict[str, any]]:
        """
        Extrait les mots-cl√©s fran√ßais d'un texte avec spaCy
        """
        cleaned_text = self.clean_french_text(text)
        doc = self.nlp(cleaned_text)

        # Extraire les tokens pertinents
        word_info = defaultdict(
            lambda: {"count": 0, "pos_tags": set(), "lemma": "", "contexts": []}
        )

        for token in doc:
            # Filtres de base pour le fran√ßais
            if (
                    token.is_stop
                    or token.is_punct
                    or token.is_space
                    or len(token.text) < self.min_word_length
                    or len(token.text) > self.max_word_length
                    or token.pos_ not in self.valid_pos
            ):
                continue

            # Lemme en minuscules
            lemma = token.lemma_.lower()

            # Ignorer les mots de la blacklist fran√ßaise
            if lemma in self.french_blacklist:
                continue

            # Ignorer les mots principalement num√©riques
            if re.match(r"^[\d\.\,]+$", lemma):
                continue

            # Ignorer les mots trop courts apr√®s lemmatisation
            if len(lemma) < 3:
                continue

            # Collecter les informations
            word_info[lemma]["count"] += 1
            word_info[lemma]["pos_tags"].add(token.pos_)
            word_info[lemma]["lemma"] = lemma

            # Contexte (phrase contenant le mot)
            sentence = token.sent.text.strip()
            if len(sentence) > 20 and len(word_info[lemma]["contexts"]) < 3:
                word_info[lemma]["contexts"].append(sentence)

        # Filtrer par fr√©quence minimale et convertir en liste
        french_keywords = []
        for word, info in word_info.items():
            if info["count"] >= self.min_frequency:
                french_keywords.append(
                    {
                        "word": word,
                        "frequency": info["count"],
                        "pos_tags": list(info["pos_tags"]),
                        "contexts": info["contexts"],
                        "importance_score": self.calculate_french_importance_score(
                            word, info, len(doc)
                        ),
                    }
                )

        # Trier par score d'importance
        french_keywords.sort(key=lambda x: x["importance_score"], reverse=True)

        return french_keywords[:top_k]

    def calculate_french_importance_score(
            self, word: str, info: dict, doc_length: int
    ) -> float:
        """
        Calcule un score d'importance pour un mot-cl√© fran√ßais
        """
        # Score de base = fr√©quence normalis√©e
        base_score = info["count"] / doc_length * 1000

        # Bonus pour les mots plus longs (plus sp√©cifiques en fran√ßais)
        length_bonus = min(len(word) / 12, 1.0)  # Ajust√© pour le fran√ßais

        # Bonus selon le type de POS en fran√ßais
        pos_bonus = 0
        if "PROPN" in info["pos_tags"]:  # Noms propres = tr√®s importants
            pos_bonus = 0.6
        elif "NOUN" in info["pos_tags"]:  # Noms = importants
            pos_bonus = 0.4
        elif "ADJ" in info["pos_tags"]:  # Adjectifs = moyennement importants
            pos_bonus = 0.3
        elif "VERB" in info["pos_tags"]:  # Verbes = moins importants
            pos_bonus = 0.2

        # Bonus pour les mots avec suffixes fran√ßais complexes
        complex_suffixes = [
            "tion", "sion", "isme", "ique", "eur", "euse", "teur", "trice",
            "ance", "ence", "it√©", "ment", "able", "ible"
        ]
        suffix_bonus = 0.2 if any(word.endswith(suffix) for suffix in complex_suffixes) else 0

        return base_score + length_bonus + pos_bonus + suffix_bonus

    def classify_french_difficulty_level(
            self, word: str, context: str, article_text: str
    ) -> str:
        """
        Classifie le niveau de difficult√© d'un mot fran√ßais pour apprenants anglophones
        """
        # 1. Longueur du mot (fran√ßais a des mots plus longs en moyenne)
        if len(word) <= 5:
            length_score = 1
        elif len(word) <= 10:
            length_score = 2
        else:
            length_score = 3

        # 2. Complexit√© morphologique fran√ßaise
        complex_endings = [
            "tion", "sion", "ment", "isme", "ique", "eur", "euse",
            "teur", "trice", "ance", "ence", "it√©", "able", "ible"
        ]

        morphology_score = 1
        for ending in complex_endings:
            if word.endswith(ending):
                morphology_score = 3
                break

        # 3. Complexit√© de l'article (Flesch Reading Ease adapt√© au fran√ßais)
        try:
            flesch_score = flesch_reading_ease(article_text)
            # Ajustement pour le fran√ßais (g√©n√©ralement plus bas qu'en anglais)
            if flesch_score >= 50:  # Facile √† lire
                article_difficulty = 1
            elif flesch_score >= 20:  # Moyennement difficile
                article_difficulty = 2
            else:  # Difficile
                article_difficulty = 3
        except Exception as e:
            logger.error(f"‚ùå Erreur calcul Flesch fran√ßais: {e}")
            article_difficulty = 2

        # 4. Sp√©cificit√© du domaine fran√ßais
        domain_patterns = {
            "it": [
                r".*logiciel.*", r".*num√©rique.*", r".*informatique.*",
                r".*technolog.*", r".*algorithme.*", r".*donn√©es.*",
                r".*d√©velopp.*", r".*program.*", r".*syst√®me.*"
            ],
            "work": [
                r".*entreprise.*", r".*management.*", r".*strat√©gie.*",
                r".*commercial.*", r".*√©conomique.*", r".*professionnel.*",
                r".*gestio.*", r".*direction.*", r".*business.*"
            ],
            "travel": [
                r".*destination.*", r".*tourisme.*", r".*voyage.*",
                r".*culture.*", r".*patrimoine.*", r".*d√©couverte.*",
                r".*aventure.*", r".*exploration.*"
            ],
            "cooking": [
                r".*cuisine.*", r".*gastronomie.*", r".*ingr√©dient.*",
                r".*recette.*", r".*culinaire.*", r".*chef.*",
                r".*plat.*", r".*saveur.*", r".*pr√©paration.*"
            ]
        }

        domain_score = 1
        if context in domain_patterns:
            for pattern in domain_patterns[context]:
                if re.match(pattern, word.lower()):
                    domain_score = 3
                    break

        # 5. Score final
        final_score = (length_score + morphology_score + article_difficulty + domain_score) / 4

        if final_score <= 1.5:
            return "beginner"
        elif final_score <= 2.5:
            return "intermediate"
        else:
            return "advanced"

    def process_french_articles_file(self, articles_file: str, context: str) -> list[dict]:
        """
        Traite un fichier d'articles fran√ßais JSONL et extrait les mots-cl√©s
        """
        logger.info(f"üîç Traitement du fichier fran√ßais: {articles_file}")

        if not os.path.exists(articles_file):
            logger.error(f"‚ùå Fichier non trouv√©: {articles_file}")
            return []

        all_keywords = []
        word_global_count = Counter()

        # Charger les articles fran√ßais
        with open(articles_file, encoding="utf-8") as f:
            articles = [json.loads(line) for line in f if line.strip()]

        logger.info(f"üìÑ {len(articles)} articles fran√ßais √† traiter")

        # Traiter chaque article
        for i, article in enumerate(articles):
            try:
                title = article.get("title", "")
                content = article.get("content", "")
                full_text = f"{title}\n\n{content}"

                # Ignorer les articles trop courts (fran√ßais n√©cessite plus de mots)
                if len(full_text.split()) < 100:
                    continue

                # Extraire les mots-cl√©s fran√ßais
                keywords = self.extract_french_keywords(full_text, top_k=25)

                # Enrichir avec m√©tadonn√©es fran√ßaises
                for kw in keywords:
                    difficulty = self.classify_french_difficulty_level(
                        kw["word"], context, full_text
                    )

                    all_keywords.append(
                        {
                            "word": kw["word"],
                            "context": context,
                            "level": difficulty,
                            "frequency_in_article": kw["frequency"],
                            "importance_score": kw["importance_score"],
                            "pos_tags": kw["pos_tags"],
                            "contexts": kw["contexts"],
                            "source_url": article.get("url", ""),
                            "source_title": title,
                            "article_index": i,
                            "language": "french",
                        }
                    )

                    word_global_count[kw["word"]] += kw["frequency"]

                if (i + 1) % 10 == 0:
                    logger.info(f"‚úÖ {i + 1}/{len(articles)} articles fran√ßais trait√©s")

            except Exception as e:
                logger.error(f"‚ùå Erreur article fran√ßais {i}: {e}")
                continue

        # Ajouter la fr√©quence globale et filtrer les doublons
        final_keywords = {}
        for kw in all_keywords:
            word = kw["word"]
            kw["global_frequency"] = word_global_count[word]

            # Garder le meilleur exemple de chaque mot
            if (
                    word not in final_keywords
                    or kw["importance_score"] > final_keywords[word]["importance_score"]
            ):
                final_keywords[word] = kw

        # Convertir en liste et trier
        result = list(final_keywords.values())
        result.sort(key=lambda x: x["global_frequency"], reverse=True)

        logger.info(f"üéØ {len(result)} mots-cl√©s fran√ßais uniques extraits pour '{context}'")
        return result

    def save_french_keywords(self, keywords: list[dict], output_file: str):
        """Sauvegarde les mots-cl√©s fran√ßais dans un fichier JSON"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(keywords, f, ensure_ascii=False, indent=2)

        logger.info(f"üíæ {len(keywords)} mots-cl√©s fran√ßais sauvegard√©s dans {output_file}")


def main():
    """Fonction principale pour extraire les mots-cl√©s fran√ßais"""
    extractor = FrenchKeywordExtractor()

    # R√©pertoires fran√ßais
    articles_dir = "data/articles_fr"
    keywords_dir = "datasets/keywords_fr"
    os.makedirs(keywords_dir, exist_ok=True)

    # Traiter chaque contexte fran√ßais
    contexts = ["it", "work", "travel", "cooking"]

    for context in contexts:
        articles_file = f"{articles_dir}/{context}/articles.jsonl"

        if os.path.exists(articles_file):
            logger.info(f"üöÄ Extraction des mots-cl√©s fran√ßais pour '{context}'")

            keywords = extractor.process_french_articles_file(articles_file, context)

            if keywords:
                output_file = f"{keywords_dir}/{context}_keywords_fr.json"
                extractor.save_french_keywords(keywords, output_file)

                # Statistiques par niveau
                levels = Counter(kw["level"] for kw in keywords)
                logger.info(
                    f"üìä Distribution des niveaux fran√ßais pour '{context}': {dict(levels)}"
                )
            else:
                logger.warning(f"‚ö†Ô∏è Aucun mot-cl√© fran√ßais extrait pour '{context}'")
        else:
            logger.warning(f"‚ö†Ô∏è Fichier d'articles fran√ßais non trouv√©: {articles_file}")

    logger.info("üéâ Extraction fran√ßaise termin√©e pour tous les contextes")


if __name__ == "__main__":
    main()
