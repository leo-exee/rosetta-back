import json
import logging
import time
from pathlib import Path

import numpy as np
import torch
from sklearn.metrics import accuracy_score, f1_score
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    Trainer,
    TrainingArguments,
    pipeline,
)

from datasets import Dataset, DatasetDict

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FrenchAIModelsTrainer:
    """
    Entra√Æneur pour les trois mod√®les d'IA fran√ßais :
    1. Fill-in-the-Blank Generator (T5 ou mT5 pour le fran√ßais)
    2. Sentence Scrambler (T5 ou mT5 pour le fran√ßais)
    3. Definition Matcher (CamemBERT ou mBERT pour le fran√ßais)
    """

    def __init__(self, base_output_dir: str = "models/trained_fr"):
        self.base_output_dir = Path(base_output_dir)
        self.base_output_dir.mkdir(parents=True, exist_ok=True)

        # Configuration des mod√®les adapt√©s au fran√ßais
        self.model_configs = {
            "fill_in_blank": {
                "model_name": "google/mt5-small",  # Mod√®le multilingue incluant le fran√ßais
                "type": "seq2seq",
                "task_prefix": "Compl√©tez les trous en fran√ßais: ",
                "max_input_length": 256,
                "max_target_length": 128,
            },
            "sentence_scrambler": {
                "model_name": "google/mt5-small",  # Mod√®le multilingue incluant le fran√ßais
                "type": "seq2seq",
                "task_prefix": "Remettez les mots fran√ßais en ordre: ",
                "max_input_length": 128,
                "max_target_length": 128,
            },
            "definition_matcher": {
                "model_name": "camembert-base",  # Mod√®le BERT fran√ßais
                "type": "classification",
                "num_labels": 3,  # 3 mots √† matcher
                "max_input_length": 256,
            },
        }

        # Statistiques d'entra√Ænement
        self.french_training_stats = {}

    def load_french_datasets(
            self, dataset_dir: str = "datasets/training_fr"
    ) -> dict[str, DatasetDict]:
        """Charge tous les datasets d'entra√Ænement fran√ßais"""
        logger.info("üìö Chargement des datasets fran√ßais...")

        datasets = {}
        dataset_path = Path(dataset_dir)

        for model_name in self.model_configs.keys():
            model_dir = dataset_path / model_name

            if not model_dir.exists():
                logger.warning(f"‚ö†Ô∏è Dossier fran√ßais non trouv√©: {model_dir}")
                continue

            # Charger train/val/test fran√ßais
            splits = {}
            for split in ["train", "val", "test"]:
                split_file = model_dir / f"{split}.jsonl"
                if split_file.exists():
                    with open(split_file, encoding="utf-8") as f:
                        data = [json.loads(line) for line in f if line.strip()]
                    splits[split] = data
                    logger.info(f"‚úÖ {model_name}/{split} fran√ßais: {len(data)} exemples")
                else:
                    logger.warning(f"‚ö†Ô∏è Fichier fran√ßais manquant: {split_file}")

            if splits:
                datasets[model_name] = splits

        return datasets

    def prepare_french_fill_in_blank_data(self, data: list[dict]) -> Dataset:
        """Pr√©pare les donn√©es fran√ßaises pour le mod√®le Fill-in-the-Blank"""
        inputs = []
        targets = []

        for item in data:
            # Format d'entr√©e: context|level|original_text
            input_parts = item["input"].split("|")
            if len(input_parts) != 3:
                continue

            context, level, original_text = input_parts

            # Format de sortie: text_with_blanks|||words_to_fill|||complete_text
            output_parts = item["output"].split("|||")
            if len(output_parts) != 3:
                continue

            text_with_blanks, words_to_fill, complete_text = output_parts

            # Input pour le mod√®le: instruction fran√ßaise + contexte + texte √† trous
            model_input = f"Contexte: {context}, Niveau: {level}. Compl√©tez les trous: {text_with_blanks}. Options: {words_to_fill}"

            # Target: texte fran√ßais complet
            model_target = complete_text

            inputs.append(model_input)
            targets.append(model_target)

        return Dataset.from_dict({"input_text": inputs, "target_text": targets})

    def prepare_french_sentence_scrambler_data(self, data: list[dict]) -> Dataset:
        """Pr√©pare les donn√©es fran√ßaises pour le mod√®le Sentence Scrambler"""
        inputs = []
        targets = []

        for item in data:
            # Format d'entr√©e: context|level|original_sentence
            input_parts = item["input"].split("|")
            if len(input_parts) != 3:
                continue

            context, level, original_sentence = input_parts

            # Format de sortie: scrambled_words|||original_sentence
            output_parts = item["output"].split("|||")
            if len(output_parts) != 2:
                continue

            scrambled_words, target_sentence = output_parts

            # Input pour le mod√®le: instruction fran√ßaise + mots m√©lang√©s
            model_input = (
                f"Contexte: {context}, Niveau: {level}. Remettez en ordre: {scrambled_words}"
            )

            # Target: phrase fran√ßaise correcte
            model_target = target_sentence

            inputs.append(model_input)
            targets.append(model_target)

        return Dataset.from_dict({"input_text": inputs, "target_text": targets})

    def prepare_french_definition_matcher_data(self, data: list[dict]) -> Dataset:
        """Pr√©pare les donn√©es fran√ßaises pour le mod√®le Definition Matcher"""
        inputs = []
        labels = []

        for item in data:
            # Format d'entr√©e: context|level
            input_parts = item["input"].split("|")
            if len(input_parts) != 2:
                continue

            context, level = input_parts

            # Format de sortie: word1,word2,word3|||def1|||def2|||def3|||1,2,3
            output_parts = item["output"].split("|||")
            if len(output_parts) != 5:
                continue

            words_str, def1, def2, def3, correct_matches = output_parts
            words = words_str.split(",")
            definitions = [def1, def2, def3]
            matches = [
                int(x) - 1 for x in correct_matches.split(",")
            ]  # Convert to 0-based

            # Cr√©er des exemples pour chaque association mot fran√ßais-d√©finition
            for i, (word, correct_def_idx) in enumerate(
                    zip(words, matches, strict=True)
            ):
                # Input: contexte fran√ßais + mot + toutes les d√©finitions
                all_defs = " | ".join(definitions)
                model_input = f"Contexte: {context}, Niveau: {level}. Mot fran√ßais: {word}. D√©finitions: {all_defs}"

                # Label: index de la bonne d√©finition
                label = correct_def_idx

                inputs.append(model_input)
                labels.append(label)

        return Dataset.from_dict({"input_text": inputs, "labels": labels})

    def tokenize_french_seq2seq_data(
            self, dataset: Dataset, tokenizer, config: dict
    ) -> Dataset:
        """Tokenise les donn√©es fran√ßaises pour les mod√®les seq2seq"""

        def tokenize_function(examples):
            # Tokeniser les inputs fran√ßais
            model_inputs = tokenizer(
                [config["task_prefix"] + text for text in examples["input_text"]],
                max_length=config["max_input_length"],
                truncation=True,
                padding="max_length",
            )

            # Tokeniser les targets fran√ßais
            targets = tokenizer(
                examples["target_text"],
                max_length=config["max_target_length"],
                truncation=True,
                padding="max_length",
            )

            model_inputs["labels"] = targets["input_ids"]
            return model_inputs

        return dataset.map(tokenize_function, batched=True)

    def tokenize_french_classification_data(
            self, dataset: Dataset, tokenizer, config: dict
    ) -> Dataset:
        """Tokenise les donn√©es fran√ßaises pour le mod√®le de classification"""

        def tokenize_function(examples):
            return tokenizer(
                examples["input_text"],
                max_length=config["max_input_length"],
                truncation=True,
                padding="max_length",
            )

        return dataset.map(tokenize_function, batched=True)

    def compute_french_metrics_seq2seq(self, eval_pred):
        """Calcule les m√©triques pour les mod√®les seq2seq fran√ßais"""
        predictions, labels = eval_pred

        # D√©coder les pr√©dictions et labels fran√ßais
        decoded_preds = self.current_tokenizer.batch_decode(
            predictions, skip_special_tokens=True
        )
        labels = np.where(labels != -100, labels, self.current_tokenizer.pad_token_id)
        decoded_labels = self.current_tokenizer.batch_decode(
            labels, skip_special_tokens=True
        )

        # Calculer l'exactitude exacte (exact match) pour le fran√ßais
        exact_matches = sum(
            pred.strip().lower() == label.strip().lower()
            for pred, label in zip(decoded_preds, decoded_labels, strict=True)
        )
        exact_match_score = exact_matches / len(decoded_preds)

        # Calculer BLEU approximatif pour le fran√ßais (simple overlap)
        bleu_scores = []
        for pred, label in zip(decoded_preds, decoded_labels, strict=True):
            pred_words = set(pred.lower().split())
            label_words = set(label.lower().split())
            if len(label_words) > 0:
                overlap = len(pred_words & label_words) / len(label_words)
                bleu_scores.append(overlap)

        avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0

        return {"exact_match": exact_match_score, "bleu_approx": avg_bleu}

    def compute_french_metrics_classification(self, eval_pred):
        """Calcule les m√©triques pour le mod√®le de classification fran√ßais"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)

        return {
            "accuracy": accuracy_score(labels, predictions),
            "f1": f1_score(labels, predictions, average="weighted"),
        }

    def train_french_seq2seq_model(
            self, model_name: str, train_dataset: Dataset, val_dataset: Dataset
    ) -> tuple[object, object]:
        """Entra√Æne un mod√®le seq2seq fran√ßais (mT5)"""
        logger.info(f"üöÄ Entra√Ænement du mod√®le fran√ßais {model_name}...")

        config = self.model_configs[model_name]

        # Charger le mod√®le et tokenizer multilingue
        tokenizer = AutoTokenizer.from_pretrained(config["model_name"])
        model = AutoModelForSeq2SeqLM.from_pretrained(config["model_name"])

        # Ajouter un token de padding si n√©cessaire
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # Stocker le tokenizer pour les m√©triques
        self.current_tokenizer = tokenizer

        # Tokeniser les donn√©es fran√ßaises
        train_tokenized = self.tokenize_french_seq2seq_data(train_dataset, tokenizer, config)
        val_tokenized = self.tokenize_french_seq2seq_data(val_dataset, tokenizer, config)

        # Configuration d'entra√Ænement adapt√©e au fran√ßais
        training_args = Seq2SeqTrainingArguments(
            output_dir=str(self.base_output_dir / model_name),
            num_train_epochs=4,  # Plus d'√©poques pour le fran√ßais
            per_device_train_batch_size=2,  # Batch size r√©duite pour mT5
            per_device_eval_batch_size=2,
            gradient_accumulation_steps=4,  # Compenser la r√©duction du batch size
            warmup_steps=200,
            weight_decay=0.01,
            logging_dir=str(self.base_output_dir / model_name / "logs"),
            logging_steps=50,
            eval_steps=200,
            save_steps=400,
            eval_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="exact_match",
            greater_is_better=True,
            predict_with_generate=True,
            generation_max_length=config["max_target_length"],
            remove_unused_columns=False,
            dataloader_pin_memory=False,
            fp16=torch.cuda.is_available(),  # Utiliser FP16 si GPU disponible
            report_to=[],  # D√©sactiver wandb par d√©faut
        )

        # Data collator
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=tokenizer, model=model, padding=True
        )

        # Trainer
        trainer = Seq2SeqTrainer(
            model=model,
            args=training_args,
            train_dataset=train_tokenized,
            eval_dataset=val_tokenized,
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_french_metrics_seq2seq,
        )

        # Entra√Ænement
        start_time = time.time()
        trainer.train()
        training_time = time.time() - start_time

        # Sauvegarder le mod√®le fran√ßais
        trainer.save_model()
        tokenizer.save_pretrained(str(self.base_output_dir / model_name))

        # Statistiques
        self.french_training_stats[model_name] = {
            "training_time": training_time,
            "train_samples": len(train_dataset),
            "val_samples": len(val_dataset),
            "final_metrics": (
                trainer.state.log_history[-1] if trainer.state.log_history else {}
            ),
            "model_type": "seq2seq",
            "language": "french",
        }

        logger.info(f"‚úÖ {model_name} fran√ßais entra√Æn√© en {training_time:.2f}s")
        return model, tokenizer

    def train_french_classification_model(
            self, model_name: str, train_dataset: Dataset, val_dataset: Dataset
    ) -> tuple[object, object]:
        """Entra√Æne un mod√®le de classification fran√ßais (CamemBERT)"""
        logger.info(f"üöÄ Entra√Ænement du mod√®le fran√ßais {model_name}...")

        config = self.model_configs[model_name]

        # Charger le mod√®le et tokenizer fran√ßais
        tokenizer = AutoTokenizer.from_pretrained(config["model_name"])
        model = AutoModelForSequenceClassification.from_pretrained(
            config["model_name"], num_labels=config["num_labels"]
        )

        # Tokeniser les donn√©es fran√ßaises
        train_tokenized = self.tokenize_french_classification_data(
            train_dataset, tokenizer, config
        )
        val_tokenized = self.tokenize_french_classification_data(
            val_dataset, tokenizer, config
        )

        # Configuration d'entra√Ænement pour CamemBERT
        training_args = TrainingArguments(
            output_dir=str(self.base_output_dir / model_name),
            num_train_epochs=3,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            gradient_accumulation_steps=2,
            warmup_steps=100,
            weight_decay=0.01,
            logging_dir=str(self.base_output_dir / model_name / "logs"),
            logging_steps=50,
            eval_steps=200,
            save_steps=400,
            eval_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="accuracy",
            greater_is_better=True,
            dataloader_pin_memory=False,
            fp16=torch.cuda.is_available(),
            report_to=[],
        )

        # Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_tokenized,
            eval_dataset=val_tokenized,
            tokenizer=tokenizer,
            compute_metrics=self.compute_french_metrics_classification,
        )

        # Entra√Ænement
        start_time = time.time()
        trainer.train()
        training_time = time.time() - start_time

        # Sauvegarder le mod√®le fran√ßais
        trainer.save_model()
        tokenizer.save_pretrained(str(self.base_output_dir / model_name))

        # Statistiques
        self.french_training_stats[model_name] = {
            "training_time": training_time,
            "train_samples": len(train_dataset),
            "val_samples": len(val_dataset),
            "final_metrics": (
                trainer.state.log_history[-1] if trainer.state.log_history else {}
            ),
            "model_type": "classification",
            "language": "french",
        }

        logger.info(f"‚úÖ {model_name} fran√ßais entra√Æn√© en {training_time:.2f}s")
        return model, tokenizer

    def evaluate_french_model(self, model_name: str, test_data: list[dict]) -> dict:
        """√âvalue un mod√®le fran√ßais entra√Æn√© sur les donn√©es de test"""
        logger.info(f"üß™ √âvaluation du mod√®le fran√ßais {model_name}...")

        model_path = self.base_output_dir / model_name
        if not model_path.exists():
            logger.error(f"‚ùå Mod√®le fran√ßais non trouv√©: {model_path}")
            return {}

        config = self.model_configs[model_name]

        try:
            if config["type"] == "seq2seq":
                # Mod√®le seq2seq fran√ßais
                pipe = pipeline(
                    "text2text-generation",
                    model=str(model_path),
                    tokenizer=str(model_path),
                    max_length=config["max_target_length"],
                    device=-1,  # Force CPU
                )

                # Pr√©parer les donn√©es de test fran√ßaises
                if model_name == "fill_in_blank":
                    test_dataset = self.prepare_french_fill_in_blank_data(test_data)
                else:  # sentence_scrambler
                    test_dataset = self.prepare_french_sentence_scrambler_data(test_data)

                # √âvaluer sur √©chantillon fran√ßais
                correct = 0
                total = 0

                for i in range(min(50, len(test_dataset))):
                    input_text = config["task_prefix"] + test_dataset[i]["input_text"]
                    target = test_dataset[i]["target_text"]

                    try:
                        prediction = pipe(input_text)[0]["generated_text"]

                        # Comparaison insensible √† la casse pour le fran√ßais
                        if prediction.strip().lower() == target.strip().lower():
                            correct += 1
                    except Exception as e:
                        logger.warning(f"Erreur pr√©diction: {e}")

                    total += 1

                accuracy = correct / total if total > 0 else 0
                return {
                    "accuracy": accuracy,
                    "total_tested": total,
                    "correct_predictions": correct,
                    "language": "french"
                }

            else:  # classification
                # Mod√®le de classification fran√ßais
                pipe = pipeline(
                    "text-classification",
                    model=str(model_path),
                    tokenizer=str(model_path),
                    device=-1,
                )

                test_dataset = self.prepare_french_definition_matcher_data(test_data)

                correct = 0
                total = 0

                for i in range(min(50, len(test_dataset))):
                    input_text = test_dataset[i]["input_text"]
                    true_label = test_dataset[i]["labels"]

                    try:
                        prediction = pipe(input_text)
                        predicted_label = int(prediction[0]["label"].split("_")[-1])

                        if predicted_label == true_label:
                            correct += 1
                    except Exception as e:
                        logger.warning(f"Erreur classification: {e}")

                    total += 1

                accuracy = correct / total if total > 0 else 0
                return {
                    "accuracy": accuracy,
                    "total_tested": total,
                    "correct_predictions": correct,
                    "language": "french"
                }

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'√©valuation fran√ßaise de {model_name}: {e}")
            return {"error": str(e), "language": "french"}

    def train_all_french_models(self, dataset_dir: str = "datasets/training_fr"):
        """Entra√Æne tous les mod√®les fran√ßais"""
        logger.info("üöÄ D√©marrage de l'entra√Ænement de tous les mod√®les fran√ßais...")

        # V√©rifier la disponibilit√© de GPU
        if torch.cuda.is_available():
            logger.info(f"üî• GPU d√©tect√© pour l'entra√Ænement fran√ßais: {torch.cuda.get_device_name()}")
        else:
            logger.info("üíª Entra√Ænement fran√ßais sur CPU")

        # Charger les datasets fran√ßais
        datasets = self.load_french_datasets(dataset_dir)

        if not datasets:
            logger.error("‚ùå Aucun dataset fran√ßais trouv√© !")
            return

        trained_models = {}

        for model_name, model_data in datasets.items():
            try:
                logger.info(f"\n{'='*50}")
                logger.info(f"üéØ Entra√Ænement fran√ßais de {model_name}")
                logger.info(f"{'='*50}")

                # Pr√©parer les datasets fran√ßais
                if model_name == "fill_in_blank":
                    train_dataset = self.prepare_french_fill_in_blank_data(model_data["train"])
                    val_dataset = self.prepare_french_fill_in_blank_data(model_data["val"])
                elif model_name == "sentence_scrambler":
                    train_dataset = self.prepare_french_sentence_scrambler_data(
                        model_data["train"]
                    )
                    val_dataset = self.prepare_french_sentence_scrambler_data(
                        model_data["val"]
                    )
                elif model_name == "definition_matcher":
                    train_dataset = self.prepare_french_definition_matcher_data(
                        model_data["train"]
                    )
                    val_dataset = self.prepare_french_definition_matcher_data(
                        model_data["val"]
                    )
                else:
                    logger.warning(f"‚ö†Ô∏è Mod√®le fran√ßais non reconnu: {model_name}")
                    continue

                # Entra√Æner le mod√®le fran√ßais
                config = self.model_configs[model_name]
                if config["type"] == "seq2seq":
                    model, tokenizer = self.train_french_seq2seq_model(
                        model_name, train_dataset, val_dataset
                    )
                else:  # classification
                    model, tokenizer = self.train_french_classification_model(
                        model_name, train_dataset, val_dataset
                    )

                trained_models[model_name] = (model, tokenizer)

                # √âvaluer sur les donn√©es de test fran√ßaises si disponibles
                if "test" in model_data:
                    eval_results = self.evaluate_french_model(model_name, model_data["test"])
                    self.french_training_stats[model_name]["test_results"] = eval_results
                    logger.info(
                        f"üìä R√©sultats de test fran√ßais pour {model_name}: {eval_results}"
                    )

            except Exception as e:
                logger.error(f"‚ùå Erreur lors de l'entra√Ænement fran√ßais de {model_name}: {e}")
                continue

        # Rapport final fran√ßais
        self.print_french_training_report()

        return trained_models

    def print_french_training_report(self):
        """Affiche un rapport complet de l'entra√Ænement fran√ßais"""
        logger.info("\n" + "=" * 60)
        logger.info("üìä RAPPORT D'ENTRA√éNEMENT FRAN√áAIS FINAL")
        logger.info("=" * 60)

        for model_name, stats in self.french_training_stats.items():
            logger.info(f"\nüá´üá∑ {model_name.upper()}")
            logger.info(f"   Type: {stats.get('model_type', 'unknown')}")
            logger.info(f"   Temps d'entra√Ænement: {stats['training_time']:.2f}s")
            logger.info(f"   √âchantillons train: {stats['train_samples']}")
            logger.info(f"   √âchantillons val: {stats['val_samples']}")

            if "test_results" in stats:
                test_acc = stats["test_results"].get("accuracy", 0)
                logger.info(f"   Pr√©cision sur test fran√ßais: {test_acc:.2%}")

        logger.info(
            "\n‚úÖ Tous les mod√®les fran√ßais sont sauvegard√©s dans: " + str(self.base_output_dir)
        )

    def save_french_training_config(self):
        """Sauvegarde la configuration d'entra√Ænement fran√ßais"""
        config_file = self.base_output_dir / "french_training_config.json"

        config_data = {
            "model_configs": self.model_configs,
            "training_stats": self.french_training_stats,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "language": "french",
            "target_learners": "english_speakers",
            "models_used": {
                "seq2seq": "google/mt5-small (multilingual T5)",
                "classification": "camembert-base (French BERT)"
            }
        }

        with open(config_file, "w", encoding="utf-8") as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)

        logger.info(f"üíæ Configuration fran√ßaise sauvegard√©e: {config_file}")


def main():
    """Fonction principale pour lancer l'entra√Ænement fran√ßais"""
    logger.info("üöÄ D√©marrage de l'entra√Ænement des mod√®les d'IA fran√ßais pour Rosetta")

    # Initialiser le trainer fran√ßais
    trainer = FrenchAIModelsTrainer()

    # Entra√Æner tous les mod√®les fran√ßais
    trained_models = trainer.train_all_french_models()

    # Sauvegarder la configuration fran√ßaise
    trainer.save_french_training_config()

    logger.info("üéâ Entra√Ænement fran√ßais termin√© ! Les mod√®les sont pr√™ts √† g√©n√©rer des exercices.")

    return trained_models


if __name__ == "__main__":
    main()
