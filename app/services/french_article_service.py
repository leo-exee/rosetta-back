import json
import logging
import os
import random
import time
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

# Configuration du logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Sources fran√ßaises par contexte
FRENCH_SOURCES = {
    "it": [
        "https://www.journaldunet.com/",
        "https://www.01net.com/",
        "https://www.numerama.com/",
        "https://www.lemonde.fr/pixels/",
        "https://www.usine-digitale.fr/",
        "https://www.silicon.fr/",
    ],
    "work": [
        "https://www.challenges.fr/",
        "https://www.lesechos.fr/",
        "https://www.capital.fr/",
        "https://business.lesechos.fr/",
        "https://www.hbrfrance.fr/",
        "https://start.lesechos.fr/",
    ],
    "travel": [
        "https://www.routard.com/",
        "https://www.geo.fr/",
        "https://www.petitfute.com/",
        "https://www.partir.com/",
        "https://www.lonelyplanet.fr/",
        "https://www.voyageurs-du-net.com/",
    ],
    "cooking": [
        "https://www.marmiton.org/",
        "https://www.750g.com/",
        "https://www.cuisineaz.com/",
        "https://madame.lefigaro.fr/cuisine/",
        "https://www.cuisineactuelle.fr/",
        "https://www.ptitchef.com/",
    ],
}

OUTPUT_DIR = "data/articles_fr"
os.makedirs(OUTPUT_DIR, exist_ok=True)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept-Language": "fr-FR,fr;q=0.9,en;q=0.8",
}


class FrenchArticleScraper:
    def __init__(self, max_articles_per_source: int = 50, delay_range: tuple = (1, 3)):
        self.max_articles_per_source = max_articles_per_source
        self.delay_range = delay_range
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    def fetch_html(self, url: str) -> str:
        """R√©cup√®re le contenu HTML d'une URL avec gestion d'erreurs am√©lior√©e"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return response.text
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur lors du fetch de {url}: {e}")
            return ""

    def extract_article_links(self, base_url: str, soup: BeautifulSoup) -> list[str]:
        """Extrait les liens d'articles fran√ßais"""
        links = set()
        domain = urlparse(base_url).netloc

        # S√©lecteurs CSS adapt√©s aux sites fran√ßais
        selectors = [
            'a[href*="/article"]',
            'a[href*="/actualite"]',
            'a[href*="/news"]',
            'a[href*="/actu"]',
            'a[href*="/recette"]',  # Pour les sites de cuisine
            'a[href*="/voyage"]',   # Pour les sites de voyage
            'a[href*="/tech"]',     # Pour les sites tech
            "article a",
            ".article-title a",
            ".entry-title a",
            ".post-title a",
            "h2 a",
            "h3 a",
            ".titre a",
            ".headline a",
        ]

        for selector in selectors:
            for link in soup.select(selector):
                href = link.get("href", "")
                if href:
                    full_url = urljoin(base_url, href)
                    if domain in full_url and self.is_french_article_url(full_url):
                        links.add(full_url)
                        if len(links) >= self.max_articles_per_source:
                            break
            if len(links) >= self.max_articles_per_source:
                break

        # Fallback: tous les liens si pas assez trouv√©s
        if len(links) < 10:
            for a in soup.find_all("a", href=True):
                href = a["href"]
                full_url = urljoin(base_url, href)
                if domain in full_url and self.is_french_article_url(full_url):
                    links.add(full_url)
                    if len(links) >= self.max_articles_per_source:
                        break

        return list(links)

    def is_french_article_url(self, url: str) -> bool:
        """D√©termine si une URL semble √™tre un article fran√ßais"""
        url_lower = url.lower()

        # URLs √† √©viter
        avoid_patterns = [
            "/tag/", "/tags/", "/category/", "/categorie/", "/author/", "/auteur/",
            "/page/", "/recherche/", "/search/", "/login", "/connexion",
            "/register", "/inscription", "/contact", "/a-propos", "/about",
            "/mentions-legales", "/cgu", "/privacy", "/confidentialite",
            ".pdf", ".jpg", ".png", ".gif", ".mp4", ".zip",
            "#", "javascript:", "mailto:", "tel:",
            "/newsletter", "/rss", "/feed"
        ]

        if any(pattern in url_lower for pattern in avoid_patterns):
            return False

        # URLs probablement int√©ressantes (mots-cl√©s fran√ßais)
        good_patterns = [
            "/article", "/actualite", "/actu", "/news", "/info",
            "/recette", "/cuisine", "/voyage", "/destination",
            "/tech", "/numerique", "/digital", "/innovation",
            "/business", "/economie", "/entreprise", "/management",
            "/2024/", "/2025/",
            "/comment-", "/pourquoi-", "/que-", "/qui-",
        ]

        return (
                any(pattern in url_lower for pattern in good_patterns)
                or len(url.split("/")) >= 4
        )

    def extract_article_content(self, soup: BeautifulSoup) -> dict[str, str]:
        """Extrait le titre et le contenu d'un article fran√ßais"""
        # Extraction du titre
        title = ""
        title_selectors = [
            "h1",
            ".entry-title",
            ".post-title",
            ".article-title",
            ".titre",
            ".headline",
            ".title",
            "title",
        ]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text().strip()
                break

        # Extraction du contenu
        content = ""

        # Supprimer les √©l√©ments ind√©sirables
        for element in soup([
            "script", "style", "nav", "header", "footer", "aside",
            "advertisement", ".pub", ".publicite", ".ad", ".ads",
            ".newsletter", ".social", ".partage", ".share",
            ".comments", ".commentaires", ".comment-form"
        ]):
            element.decompose()

        # Chercher le contenu principal (s√©lecteurs fran√ßais)
        content_selectors = [
            ".entry-content",
            ".post-content",
            ".article-content",
            ".contenu-article",
            ".article-body",
            ".content",
            ".contenu",
            "article",
            ".story-body",
            ".texte-article",
            "main",
            ".main-content",
        ]

        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                paragraphs = content_elem.find_all("p")
                if paragraphs:
                    content = "\n".join(
                        p.get_text().strip() for p in paragraphs if p.get_text().strip()
                    )
                    break

        # Fallback: tous les paragraphes
        if not content:
            paragraphs = soup.find_all("p")
            content = "\n".join(
                p.get_text().strip() for p in paragraphs if p.get_text().strip()
            )

        return {"title": title, "content": content.strip()}

    def scrape_source(self, context: str, source_url: str) -> list[dict]:
        """Scrape une source fran√ßaise sp√©cifique"""
        logger.info(f"üîç Scraping {source_url} pour le contexte fran√ßais '{context}'")

        # R√©cup√©rer la page principale
        html = self.fetch_html(source_url)
        if not html:
            logger.warning(f"‚ùå Impossible de r√©cup√©rer {source_url}")
            return []

        soup = BeautifulSoup(html, "html.parser")
        article_links = self.extract_article_links(source_url, soup)

        logger.info(f"üìÑ {len(article_links)} liens d'articles fran√ßais trouv√©s")

        articles = []
        for i, link in enumerate(article_links):
            try:
                # D√©lai al√©atoire pour √©viter le rate limiting
                time.sleep(random.uniform(*self.delay_range))

                article_html = self.fetch_html(link)
                if not article_html:
                    continue

                article_soup = BeautifulSoup(article_html, "html.parser")
                article_data = self.extract_article_content(article_soup)

                # Filtrer les articles trop courts (fran√ßais n√©cessite plus de mots)
                if len(article_data["content"].split()) < 150:
                    logger.debug(f"‚ö†Ô∏è Article fran√ßais trop court ignor√©: {link}")
                    continue

                # V√©rifier que c'est bien du fran√ßais (heuristique simple)
                if not self.is_likely_french_text(article_data["content"]):
                    logger.debug(f"‚ö†Ô∏è Article probablement pas en fran√ßais: {link}")
                    continue

                articles.append(
                    {
                        "context": context,
                        "source": urlparse(source_url).netloc,
                        "url": link,
                        "title": article_data["title"],
                        "content": article_data["content"],
                        "language": "french",
                        "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    }
                )

                logger.info(
                    f"‚úÖ Article fran√ßais {i+1}/{len(article_links)} trait√©: {article_data['title'][:50]}..."
                )

            except Exception as e:
                logger.error(f"‚ùå Erreur lors du traitement de {link}: {e}")
                continue

        logger.info(f"üéØ {len(articles)} articles fran√ßais valides r√©cup√©r√©s de {source_url}")
        return articles

    def is_likely_french_text(self, text: str) -> bool:
        """Heuristique simple pour d√©tecter du texte fran√ßais"""
        if not text or len(text) < 100:
            return False

        # Mots fran√ßais courants
        french_words = [
            "le", "de", "et", "√†", "un", "il", "√™tre", "et", "en", "avoir",
            "que", "pour", "dans", "ce", "son", "une", "sur", "avec", "ne",
            "se", "pas", "tout", "plus", "par", "grand", "comme", "mais",
            "dans", "cette", "des", "les", "du", "la", "leur", "ses",
            "fran√ßais", "france", "aussi", "tr√®s", "nous", "vous", "ils"
        ]

        # Compter les mots fran√ßais
        words = text.lower().split()
        french_count = sum(1 for word in words[:100] if any(fw in word for fw in french_words))

        # Au moins 20% de mots fran√ßais parmi les 100 premiers
        return french_count >= 20

    def scrape_context(self, context: str, sources: list[str]) -> None:
        """Scrape toutes les sources d'un contexte fran√ßais"""
        logger.info(f"üöÄ D√©but du scraping fran√ßais pour le contexte '{context}'")

        all_articles = []

        for source in sources:
            articles = self.scrape_source(context, source)
            all_articles.extend(articles)

        # Sauvegarder les r√©sultats
        context_dir = f"{OUTPUT_DIR}/{context}"
        os.makedirs(context_dir, exist_ok=True)

        output_file = f"{context_dir}/articles.jsonl"
        with open(output_file, "w", encoding="utf-8") as f:
            for article in all_articles:
                f.write(json.dumps(article, ensure_ascii=False) + "\n")

        logger.info(f"üíæ {len(all_articles)} articles fran√ßais sauvegard√©s dans {output_file}")
        logger.info(
            f"‚ú® Scraping fran√ßais termin√© pour '{context}': {len(all_articles)} articles au total"
        )


def main():
    """Fonction principale pour le scraping fran√ßais"""
    scraper = FrenchArticleScraper(max_articles_per_source=50, delay_range=(2, 4))

    for context, sources in FRENCH_SOURCES.items():
        try:
            scraper.scrape_context(context, sources)
            # Pause entre les contextes
            time.sleep(5)
        except KeyboardInterrupt:
            logger.info("üõë Arr√™t demand√© par l'utilisateur")
            break
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du scraping du contexte fran√ßais '{context}': {e}")
            continue

    logger.info("üéâ Scraping fran√ßais termin√© pour tous les contextes")


if __name__ == "__main__":
    main()
